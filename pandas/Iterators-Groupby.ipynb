{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Stream larger-than-memory data through a pipeline\n",
    "- Composable thanks to the iterator protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My favorite \"feature\" of pandas is that it's written in Python.\n",
    "Python has great language-level features for handling streams of data\n",
    "that may not fit in memory.\n",
    "This can be a useful pre-processing step to reading the data into a DataFrame or\n",
    "NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from itertools import islice, takewhile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from toolz import partition_all, partitionby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "sns.set(context='talk')\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beer Reviews Dataset\n",
    "\n",
    "- A review is a list of lines\n",
    "- Each review line is formated like `meta/field: value`\n",
    "- Reviews are separated by blank lines (i.e. the line is just `'\\n'`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford has a [dataset on beer reviews](https://snap.stanford.edu/data/web-BeerAdvocate.html). The raw file is too large for me to include, but I split off a couple subsets for us to work with.\n",
    "\n",
    "Pandas can't read this file natively, but we have Python!\n",
    "We'll use Python to parse the raw file and tranform it into a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beer/name: Sausa Weizen\n",
      "beer/beerId: 47986\n",
      "beer/brewerId: 10325\n",
      "beer/ABV: 5.00\n",
      "beer/style: Hefeweizen\n",
      "review/appearance: 2.5\n",
      "review/aroma: 2\n",
      "review/palate: 1.5\n",
      "review/taste: 1.5\n",
      "review/overall: 1.5\n",
      "review/time: 1234817823\n",
      "review/profileName: stcules\n",
      "review/text: A lot of foam. But a lot.\tIn the smell some banana, and then lactic and tart. Not a good start.\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\tAgain tending to lactic sourness.\tSame for the taste. With some yeast and banana.\t\t\n",
      "\n",
      "beer/name: Red Moon\n",
      "beer/beerId: 48213\n",
      "beer/brewerId: 10325\n",
      "beer/ABV: 6.20\n",
      "beer/style: English Strong Ale\n",
      "review/appearance: 3\n",
      "review/aroma: 2.5\n",
      "review/palate: 3\n",
      "review/taste: 3\n",
      "review/overall: 3\n",
      "review/time: 1235915097\n",
      "review/profileName: stcules\n",
      "review/text: Dark red color, light beige foam, average.\tIn the smell malt and caramel, not really light.\tAgain malt and caramel in the taste, not bad in the end.\tMaybe a note of honey in teh back, and a light fruitiness.\tAverage body.\tIn the aftertaste a light bitterness, with the malt and red fruit.\tNothing exceptional, but not bad, drinkable beer.\t\t\n",
      "\n",
      "beer/name: Black Horse Black Beer\n",
      "beer/beerId: 48215\n",
      "beer/brewerId: 10325\n",
      "beer/ABV: 6.50\n",
      "beer/style: Foreign / Export Stout\n",
      "review/appearance: 3\n",
      "review/aroma: 2.5\n",
      "review/palate: 3\n",
      "review/taste: 3\n",
      "review/overall: 3\n",
      "review/time: 1235916604\n",
      "review/profileName: stcules\n",
      "review/text: Almost totally black. Beige foam, quite compact, not bad.\tLight smell, just a bit of roas\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(\"data/beer-raw-small.txt.gz\", \"r\") as f:\n",
    "    print(f.read(1500).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full compressed raw dataset is about 500MB, so reading it all into memory might not be pleasent (we're working with a small subset that would fit in memory, but pretend it didn't).\n",
    "Fortunately, Python's iterator protocol and generators make dealing with large streams of data pleasent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a solution\n",
    "\n",
    "Let's build a solution together. I'll provide some guidance as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beer/name: Sausa Weizen\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a handle to the data\n",
    "f = gzip.open(\"data/beer-raw-small.txt.gz\", \"rt\")\n",
    "f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Tasks\n",
    "\n",
    "1. split the raw text stream into individual reviews\n",
    "2. transform each individual review into a data container\n",
    "3. combine a chunk of transformed individual reviews into a collection\n",
    "4. store the chunk to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Split the text stream\n",
    "\n",
    "We'll use `toolz.partitionby`. It takes an iterator like `f`, and splits it according to `func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('beer/name: Sausa Weizen\\n',\n",
       " 'beer/beerId: 47986\\n',\n",
       " 'beer/brewerId: 10325\\n',\n",
       " 'beer/ABV: 5.00\\n',\n",
       " 'beer/style: Hefeweizen\\n',\n",
       " 'review/appearance: 2.5\\n',\n",
       " 'review/aroma: 2\\n',\n",
       " 'review/palate: 1.5\\n',\n",
       " 'review/taste: 1.5\\n',\n",
       " 'review/overall: 1.5\\n',\n",
       " 'review/time: 1234817823\\n',\n",
       " 'review/profileName: stcules\\n',\n",
       " 'review/text: A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\\t\\t\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.seek(0)  # Make the cell idempotent\n",
    "split = partitionby(lambda x: x == '\\n', f)\n",
    "a, b = next(split), next(split)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've gone from\n",
    "\n",
    "```python\n",
    "[\n",
    "    \"beer/name: Susa Weizen\\n\",\n",
    "    ...\n",
    "    \"review/text: ...\\n\",\n",
    "    \"\\n\",\n",
    "    \"beer/name: Beer 2\\n\",\n",
    "    \"...\",\n",
    "    \"review/text: ...\\n\",\n",
    "    \"\\n\",\n",
    "]\n",
    "```\n",
    "\n",
    "To\n",
    "\n",
    "```python\n",
    "[\n",
    "    (\n",
    "        \"beer/name Susa Weizen\\n\",\n",
    "        ...\n",
    "        \"review/text: ...\\n\"\n",
    "    ),\n",
    "    (\"\\n\",),\n",
    "    (\n",
    "        \"beer/name: Beer 2\\n\",\n",
    "        ...\n",
    "        \"review/text: ...\\n\"\n",
    "    ),\n",
    "    (\"\\n\",),\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can clean up those newlines with a generator expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7f9f03435120>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.seek(0)\n",
    "reviews = (x for x in partitionby(lambda x: x == \"\\n\", f)\n",
    "           if x != (\"\\n\",))\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Parse each review\n",
    "\n",
    "Let's grab out the first review, and turn it into something a bit nicer than a tuple of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('beer/name: Sausa Weizen\\n',\n",
       " 'beer/beerId: 47986\\n',\n",
       " 'beer/brewerId: 10325\\n',\n",
       " 'beer/ABV: 5.00\\n',\n",
       " 'beer/style: Hefeweizen\\n',\n",
       " 'review/appearance: 2.5\\n',\n",
       " 'review/aroma: 2\\n',\n",
       " 'review/palate: 1.5\\n',\n",
       " 'review/taste: 1.5\\n',\n",
       " 'review/overall: 1.5\\n',\n",
       " 'review/time: 1234817823\\n',\n",
       " 'review/profileName: stcules\\n',\n",
       " 'review/text: A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\\t\\t\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.seek(0);  # make the cell idempotent\n",
    "review = next(partitionby(lambda x: x == '\\n', f))\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Format Review\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Format Review</h1>\n",
    "</div>\n",
    "<p>Write a function `format_review` that converts an item like `review` into a dict</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will have one entry per line, where the are the stuff to the left of the colon and the values are the stuff to the right.\n",
    "For example, the first line would be\n",
    "\n",
    "`'beer/name: Sausa Weizen\\n',` => `'beer/name': 'Sausa Weizen'`\n",
    "\n",
    "Make sure to clean up the line endings too.\n",
    "\n",
    "- Hint: Check out the [python string methods](https://docs.python.org/3/library/stdtypes.html#string-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your function against `expected` by evaluating the next cell.\n",
    "If you get a failure, adjust your `format_review` until it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from typing import List, Dict\n",
    "\n",
    "f.seek(0);  # make the cell idempotent\n",
    "review = next(partitionby(lambda x: x == '\\n', f))\n",
    "\n",
    "\n",
    "def format_review(review):\n",
    "    \"\"\"Your code goes below\"\"\"\n",
    "    formatted = dict([line.strip('\\n').split(\": \", 1) for line in review])\n",
    "    return formatted\n",
    "    \n",
    "\n",
    "class TestFormat(unittest.TestCase):\n",
    "    maxDiff = None\n",
    "\n",
    "    def test_format_review(self):\n",
    "        result = format_review(review)\n",
    "        expected = {\n",
    "            'beer/ABV': '5.00',\n",
    "            'beer/beerId': '47986',\n",
    "            'beer/brewerId': '10325',\n",
    "            'beer/name': 'Sausa Weizen',\n",
    "            'beer/style': 'Hefeweizen',\n",
    "            'review/appearance': '2.5',\n",
    "            'review/aroma': '2',\n",
    "            'review/overall': '1.5',\n",
    "            'review/palate': '1.5',\n",
    "            'review/profileName': 'stcules',\n",
    "            'review/taste': '1.5',\n",
    "            'review/text': 'A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\\t\\t',\n",
    "            'review/time': '1234817823'\n",
    "        }\n",
    "        self.assertEqual(result, expected)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestFormat())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that optional argument to split, which controls the number of splits made; If a review text had contained a literal `': '`, we'd be in trouble since it'd get split again.\n",
    "\n",
    "Make sure you executed the above solution cell twice (first to load, second to execute) as we'll be using that `format_review` function down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To a DataFrame\n",
    "\n",
    "Assuming we've processed many reviews into a list, we'll then build up a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>brewer_id</th>\n",
       "      <th>abv</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>time</th>\n",
       "      <th>profile_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sausa Weizen</td>\n",
       "      <td>47986</td>\n",
       "      <td>10325</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hefeweizen</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2009-02-16 20:57:03</td>\n",
       "      <td>stcules</td>\n",
       "      <td>A lot of foam. But a lot.\\tIn the smell some b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      beer_name beer_id brewer_id  abv  beer_style  review_appearance  \\\n",
       "0  Sausa Weizen   47986     10325  5.0  Hefeweizen                2.5   \n",
       "\n",
       "   review_aroma  review_palate  review_taste  review_overall  \\\n",
       "0           2.0            1.5           1.5             1.5   \n",
       "\n",
       "                 time profile_name  \\\n",
       "0 2009-02-16 20:57:03      stcules   \n",
       "\n",
       "                                                text  \n",
       "0  A lot of foam. But a lot.\\tIn the smell some b...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [format_review(review)]  # imagine a list of many reviews\n",
    "\n",
    "col_names = {\n",
    "    'beer/ABV': 'abv',\n",
    "    'beer/beerId': 'beer_id',\n",
    "    'beer/brewerId': 'brewer_id',\n",
    "    'beer/name': 'beer_name',\n",
    "    'beer/style': 'beer_style',\n",
    "    'review/appearance': 'review_appearance',\n",
    "    'review/aroma': 'review_aroma',\n",
    "    'review/overall': 'review_overall',\n",
    "    'review/palate': 'review_palate',\n",
    "    'review/profileName': 'profile_name',\n",
    "    'review/taste': 'review_taste',\n",
    "    'review/text': 'text',\n",
    "    'review/time': 'time'\n",
    "}\n",
    "df = pd.DataFrame(r)\n",
    "numeric = ['abv', 'review_appearance', 'review_aroma',\n",
    "           'review_overall', 'review_palate', 'review_taste']\n",
    "df = (df.rename(columns=col_names)\n",
    "        .replace('', np.nan))\n",
    "df[numeric] = df[numeric].astype(float)\n",
    "df['time'] = pd.to_datetime(df.time.astype(int), unit='s')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, writing that as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_dataframe(reviews):\n",
    "    df = pd.DataFrame(list(reviews))\n",
    "\n",
    "    col_names = {\n",
    "        'beer/ABV': 'abv',\n",
    "        'beer/beerId': 'beer_id',\n",
    "        'beer/brewerId': 'brewer_id',\n",
    "        'beer/name': 'beer_name',\n",
    "        'beer/style': 'beer_style',\n",
    "        'review/appearance': 'review_appearance',\n",
    "        'review/aroma': 'review_aroma',\n",
    "        'review/overall': 'review_overall',\n",
    "        'review/palate': 'review_palate',\n",
    "        'review/profileName': 'profile_name',\n",
    "        'review/taste': 'review_taste',\n",
    "        'review/text': 'text',\n",
    "        'review/time': 'time'\n",
    "    }\n",
    "    order = ['brewer_id', 'beer_id', 'beer_name',\n",
    "             'beer_style', 'abv',\n",
    "             'profile_name', 'time',\n",
    "             'review_appearance', 'review_aroma',\n",
    "             'review_palate', 'review_taste',\n",
    "             'review_overall',\n",
    "             'text']\n",
    "    df = df.rename(columns=col_names)[order]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline\n",
    "\n",
    "1. `file -> review_lines : List[str]`\n",
    "2. `review_lines -> reviews : Dict[str, str]`\n",
    "3. `reviews -> DataFrames`\n",
    "4. `DataFrames -> CSV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full pipeline would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r",
      "1\r",
      "2\r",
      "3\r",
      "4\r",
      "5\r",
      "6\r",
      "7\r"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100  # Number of reviews to process per chunk\n",
    "                  # Intentionally small for demonstration    \n",
    "p = Path(\"data/beer-raw-small.txt.gz\")\n",
    "\n",
    "with gzip.open(p, \"rt\") as f:\n",
    "    \n",
    "    review_lines_and_newlines = partitionby(lambda x: x == '\\n', f)\n",
    "    # so filter out the newlines\n",
    "    review_lines = (x for x in review_lines_and_newlines if x != (\"\\n\",))\n",
    "    \n",
    "    # generator expression to go from List[str] -> Dict[str, str]\n",
    "    reviews = (format_review(x) for x in review_lines)\n",
    "    \n",
    "    # `reviews` yields one dict per review.\n",
    "    # Won't fit in memory, so do `BATCH_SIZE` per chunk\n",
    "    chunks = partition_all(BATCH_SIZE, reviews)\n",
    "    dfs = (as_dataframe(chunk) for chunk in chunks)\n",
    "\n",
    "    p.parent.joinpath(\"beer\").mkdir(exist_ok=True)\n",
    "\n",
    "    # the first time we read from disk\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.to_csv(\"data/beer/chunk_%s.csv.gz\" % i, index=False,\n",
    "                  compression=\"gzip\")\n",
    "        print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This runs comfortably in memory. At any given time, we only have `BATCH_SIZE` reviews in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Aside on [Dask](http://dask.pydata.org/en/latest/)\n",
    "\n",
    "> Dask is a flexible parallel computing library for analytic computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Postpone and read Dask in some details in future!)**\n",
    "\n",
    "The original dataset is in random order, but I wanted to select an interesting subset for us to work on: all the reviews by the top 100 reviewers.\n",
    "\n",
    "You might know enough pandas now to figure out the top-100 reviewers by count.\n",
    "Do a `value_counts` , select the top 100, then select the index.\n",
    "\n",
    "```python\n",
    "top_reviewers = df.profile_name.value_counts().nlargest(100).index\n",
    "```\n",
    "\n",
    "\n",
    "Recall that `value_counts` will be a `Series` where the index is each unique `profile_name` and the values is the count of reviews for that profile.\n",
    "We use `nlargets(100)` to get the 100 largest values, and `.index` to get the actual profile names. \n",
    "\n",
    "With that we could do an `isin` like\n",
    "\n",
    "```python\n",
    "subset = df[df.profile_name.isin(top_reviewers)\n",
    "```\n",
    "\n",
    "To get the subset of reviews that came from the 100 most active reviewers.\n",
    "\n",
    "But that assumes we have a `df` containing the full dataset in memory.\n",
    "My laptop can't load the entire dataset though (recall that we're working with a subset today).\n",
    "\n",
    "It wouldn't be *that* hard to write a custom solution in python or pandas using chunking like we did up above.\n",
    "We'd split our task into parts\n",
    "\n",
    "- read a chunk\n",
    "- compute `chunk.profile_name.value_counts()`\n",
    "- store that intermediate `value_counts` in a global container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/map-reduce-count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've processed each chunk, our final steps are to\n",
    "\n",
    "- merge each `value_counts` chunk by summing\n",
    "- filter to the top 100\n",
    "\n",
    "This pattern of processing chunks independently (map) and combining the results into a smaller output (reduce) is common, and doing it manually gets old.\n",
    "Dask can help out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections, Tasks, Schedulers\n",
    "\n",
    "![](http://dask.pydata.org/en/latest/_images/collections-schedulers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask has several components, so it can be hard to succinctly describe the library.\n",
    "Right now, we'll view it as providing \"big dataframes\" (one of its \"collections\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewer_id</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>abv</th>\n",
       "      <th>profile_name</th>\n",
       "      <th>time</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=8</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 8 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              brewer_id beer_id beer_name beer_style      abv profile_name    time review_appearance review_aroma review_palate review_taste review_overall    text\n",
       "npartitions=8                                                                                                                                                      \n",
       "                  int64   int64    object     object  float64       object  object           float64      float64       float64      float64        float64  object\n",
       "                    ...     ...       ...        ...      ...          ...     ...               ...          ...           ...          ...            ...     ...\n",
       "...                 ...     ...       ...        ...      ...          ...     ...               ...          ...           ...          ...            ...     ...\n",
       "                    ...     ...       ...        ...      ...          ...     ...               ...          ...           ...          ...            ...     ...\n",
       "                    ...     ...       ...        ...      ...          ...     ...               ...          ...           ...          ...            ...     ...\n",
       "Dask Name: read-csv, 8 tasks"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv(\"data/beer/chunk*.csv.gz\", compression=\"gzip\", blocksize=None,\n",
    "                 parse_dates=['time'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That API should look familiar to you, now that you're experienced pandas users.\n",
    "We swap out `pd` for `dd`, and stuff mostly just works.\n",
    "Occasionally, you'll have a `dask`-specific thing like `blocksize` (number of bytes per smaller dataframe) that don't apply to pandas, which assumes things fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our \"big dataframe\", we can do normal pandas operations like `.value_counts`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    int64\n",
       "      ...\n",
       "Name: profile_name, dtype: int64\n",
       "Dask Name: value-counts-agg, 25 tasks"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_per_person = df.profile_name.value_counts()\n",
    "reviews_per_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a `dask.Series`, the dask analog to a `dask.dataframe`.\n",
    "One important point: we haven't actually done any real work yet.\n",
    "The operations on a `dask.dataframe` is really just a pandas-like API for\n",
    "manipulating the directed acylic graph (DAG) of operations, and a bit of metadata about those operations.\n",
    "We can visualize that DAG with the `.visualize` method. I've done it ahead of\n",
    "time since it uses `graphviz`, which can be a pain to install on every system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reviews_per_person.visualize(rankdir=\"LR\")\n",
    "```\n",
    "\n",
    "![](figures/reviews_per_person.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the 100 most active reviewers. There's a couple ways to do this.\n",
    "\n",
    "1. Sort `reviews_per_person`, then take the the last 100\n",
    "2. Scan `reviews_per_person`, keeps the 100 largest you've seen\n",
    "\n",
    "For large datasets, 2 is *much* easier / faster. It's implemented as `.nlargest` on pandas and dask Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Index Structure:\n",
       "npartitions=1\n",
       "    object\n",
       "       ...\n",
       "dtype: object\n",
       "Dask Name: series-nlargest-agg, 28 tasks"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_reviewers = reviews_per_person.nlargest(100).index\n",
    "top_reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we still just have a dask object (a DAG of operations, to be computed later). To actually get a concrete value, you hand the DAG off to a *scheduler*, using `compute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Halcyondays', 'Beerandraiderfan', 'ccrida', 'RedDiamond',\n",
       "       'barleywinefiend', 'Reidrover', 'Tilley4', 'msubulldog25', 'JohnGalt1',\n",
       "       'oberon', 'stcules', 'flexabull', 'BeerAdvocate', 'zoso1967',\n",
       "       'russpowell', 'merlin48', 'Proteus93', 'BuckeyeNation', 'philbe311',\n",
       "       'TheGordianKnot', 'beer2day', 'onix1agr', 'jeff1973', 'snaotheus',\n",
       "       'harrymel', 'Wetpaperbag', 'Spikester', 'ElGordo', 'Deuane',\n",
       "       'DeanMoriarty', 'augustgarage', 'Slatetank', 'Knapp85', 'philbertk',\n",
       "       'ArrogantB', 'bbothen', 'Monkeyknife', 'Mora2000', 'MrVonzipper',\n",
       "       'MuenchenerKindl', 'beagle75', 'oline73', 'UCLABrewN84',\n",
       "       'notchucknorris', 'youradhere', 'ReelBigwigFish', 'argock',\n",
       "       'northyorksammy', 'freed', 'dedrinker', 'WeisGuy', 'Thorpe429',\n",
       "       'emmasdad', 'Docer', 'ckeegan04', 'SurlyDuff', 'DoubleJ', 'Dubbercody',\n",
       "       'organicbrewer', 'WesWes', 'bump8628', 'brewandbbq', 'maximum12',\n",
       "       'woodychandler', 'BEERchitect', 'PDXHops', 'woemad', 'Phelps',\n",
       "       'beerman207', 'Bubba83', 'BeerZack', 'beertunes', 'womencantsail',\n",
       "       'Klym', 'Kegatron', 'RoyalT', 'mgbickel', 'meatyard', 'BeerSox',\n",
       "       'indiapaleale', 'blackie', 'rootbeerman', 'mdagnew', 'JayQue',\n",
       "       'mtrentm', 'akorsak', 'mcallister', 'hopdog', 'IronLover', 'Scotchboy',\n",
       "       'Seanibus', 'brentk56', 'BrewerB', 'AEK', 'mikereaser', 'mikesgroove',\n",
       "       'HopDerek', 'Arbitrator', 'zeff80', 'cswhitehorse'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_reviewers.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask.dataframe` uses the threaded scheduler, so the actual computation is done in parallel. You could also use the `multiprocessing` scheduler (if you have operations that hold the GIL), or the `distributed` scheduler if you have a cluster handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `top_reviewers` as a boolean mask, just like in regular pandas.\n",
    "\n",
    "```python\n",
    ">>> df[df.profile_name.isin(top_reviewers.compute())].to_parquet(\n",
    "    \"data/subset.parq\", compression='gzip'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to pandas\n",
    "\n",
    "I've provided the reviews by the top 100 reviewers.\n",
    "We'll use it for talking about groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"data/subset.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Reviews by Hour\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Reviews by Hour</h1>\n",
    "</div>\n",
    "\n",
    "<p>Make a barplot of the count of reviews by hour of the day.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hint: Use the `.dt` namespace to get the `hour` component of a `datetime`\n",
    "- Hint: We've seen `Series.value_counts` for getting the count of each value\n",
    "- Hint: Use `.sort_index` to make sure the data is ordered by hour, not count\n",
    "- Hint: Use the [`.plot`](http://pandas.pydata.org/pandas-docs/stable/api.html#plotting) namespace to get a `bar` chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.time.dt.hour\n",
    "   .value_counts()\n",
    "   .sort_index()\n",
    "   .plot.bar(rot=0, color='k', width=.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Pale Ales\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Pale Ales</h1>\n",
    "</div>\n",
    "<p>\n",
    "Make a variable `pale_ales` that filters `df` to just rows where `beer_style` contains the string `'pale ale'` (ignoring case)\n",
    "</p>\n",
    "- Hint: Use the `df.beer_style.str` namespace and find a method for checking whether a string contains another string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas = df[df.beer_style.str.contains('pale ale', case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby operations come up in a lot of contexts.\n",
    "At its root, groupby about doing an operation on many subsets of the data, each of which shares something in common.\n",
    "The components of a groupby operation are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a groupby\n",
    "\n",
    "1. **split** a table into groups\n",
    "2. **apply** a function to each group\n",
    "3. **combine** the results into a single DataFrame or Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas the `split` step looks like\n",
    "\n",
    "```python\n",
    "df.groupby( grouper )\n",
    "```\n",
    "\n",
    "`grouper` can be many things\n",
    "\n",
    "- Series (or string indicating a column in `df`); the Series must be of same length to the rows of the DataFrame.\n",
    "- function (to be applied on the index)\n",
    "- dict : you can specify custom grouping by value\n",
    "- `levels=[ names of levels in a MultiIndex ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Break a table into smaller logical tables according to some rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = df.groupby(\"beer_name\")\n",
    "gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really done any actual work yet, but pandas knows what it needs to know to break the larger `df` into many smaller pieces, one for each distinct `beer_name`. This `gr` variable is now a `GroupBy` object. It has not actually computed anything yet except for some intermediate data about the group key. The idea is that this object has all of the information needed to then apply some operation to each of the groups. \n",
    "\n",
    "A `DataFrame` is grouped by rows this way `axis=0`. It can also be grouped by columns as well: `axis=1`, though it is less intuitive. Even a `Categorical` object, such as those returned by `cut` or `qcut`, can be passed directly to `groupby`. Actually, `groupBy` operations can be significantly faster with categoricals because the underlying algorithms use the integer-based codes array instead of an array of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GroupBy` object supports iteration, generating a sequence of 2-tuples containing group name along with the chunk of data, or just simply create a `dict` out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in gr:\n",
    "    print(name)\n",
    "    print(group)\n",
    "\n",
    "pieces = dict(list(gr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a specific group by name, use the `.get_group()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply & Combine\n",
    "\n",
    "To finish the groupby, we apply a method to the groupby object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cols = ['review_appearance', 'review_aroma', 'review_overall',\n",
    "               'review_palate', 'review_taste']\n",
    "\n",
    "# Note that df.groupby('beer_name')[review_cols] is synthetic sugar for df[review_cols].groupby('beer_name')\n",
    "df.groupby('beer_name')[review_cols].agg('mean') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the function we applied was `'mean'`.\n",
    "Pandas has implemented cythonized versions of certain common methods like mean, sum, etc.\n",
    "You can also pass in regular functions like `np.mean`.\n",
    "\n",
    "In terms of split, apply, combine, split was `df.groupby('beer_name')`. \n",
    "We apply the `mean` function by passing in `'mean'`.\n",
    "Finally, by using the `.agg` method (for aggregate) we tell pandas to combine the results with one output row per group.\n",
    "\n",
    "You may notice that some methods like `describe` also work, even though they are not aggregations, strictly speaking. That operation is then done per group.\n",
    "\n",
    "If `review_cols` is just a string, then the result will be a `Series`; otherwise it is a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('beer_name')[review_cols].agg(np.mean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, [certain methods](http://pandas.pydata.org/pandas-docs/stable/api.html#id35) have been attached to `Groupby` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('beer_name')[review_cols].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Highest Variance\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Highest Variance</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the `beer_style`s with the greatest variance in `abv`.</p>\n",
    "\n",
    "- hint: `.var` calculates the varaince and is available on `GroupBy` objects like `gr.abv`.\n",
    "- hint: use `.sort_values` to sort a Series by the values (it took us a while to come up with that name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('beer_style').abv.var().sort_values(ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.agg` output shape\n",
    "\n",
    "The output shape is determined by the grouper, data, and aggregation\n",
    "\n",
    "- Grouper: Controls the output index (You can also pass `as_index=False` to `groupby` so that the groupers are only columns in the aggregated data, but not Index or MultiIndex).\n",
    "    * single grouper -> Index\n",
    "    * array-like grouper -> MultiIndex\n",
    "- Subject (Groupee): Controls the output data values\n",
    "    * single column -> Series (or DataFrame if multiple aggregations)\n",
    "    * multiple columns -> DataFrame\n",
    "- Aggregation: Controls the output columns\n",
    "    * single aggfunc -> Index in the colums\n",
    "    * multiple aggfuncs -> MultiIndex in the columns (Or 1-D Index if groupee is 1-D). Note: if different aggreation function needs to be done on different columns, pass a `dict` to `agg` that contains a mapping of column names to any of the function specifications listed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single grouper, single groupee, single aggregation\n",
    "df.groupby('beer_style').review_overall.agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple groupers, multiple groupee, single aggregation\n",
    "df.groupby(['brewer_id', 'beer_name'])[review_cols].agg(['mean', 'min', 'max', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Rating by length\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Rating by length</h1>\n",
    "</div>\n",
    "\n",
    "<p>Plot the relationship between review length (number of characters) and average `reveiw_overall`.</p>\n",
    "\n",
    "- Hint: use `.plot(style='k.')`\n",
    "- We've grouped by columns so far, you can also group by any series with the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is groupby Series.\n",
    "review_length = df.text.str.len()\n",
    "gr = df.groupby(review_length).review_overall\n",
    "gr.mean().plot(style='k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Reviews by Length\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Reviews by Length</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the relationship between review length (number of **words** and average `reveiw_overall`.)</p>\n",
    "\n",
    "- Hint: You can pass a [regular expression](https://docs.python.org/3/howto/regex.html#matching-characters) to any of the `.str` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(df.text.str.count('\\w'))\n",
    "   .review_overall\n",
    "   .mean().plot(style='k.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Rating by number of Reviews\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Rating by number of Reviews</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the relationship between the number of reviews for a beer and the average `review_overall`.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby('beer_id')\n",
    "   .review_overall\n",
    "   .agg(['mean', 'count'])\n",
    "   .plot.scatter(x='count', y='mean', color='k',\n",
    "                 marker='.', alpha=.25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "A *transform* is a function whose output is the same shape as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a groupby has three steps: split, apply, combine.\n",
    "So far, all of the functions we've applied have been *aggregations*: the rule for \"combine\" is one row per group.\n",
    "\n",
    "You can use `Groupby.transform` when you have an operation that should be done *groupwise*, but the result should be the same shape.\n",
    "For example, suppose we wanted to de-mean each reviewer's scores by their average score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define demean(v: array) -> array\n",
    "def demean(v):\n",
    "    return v - v.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just calling `demean` on the entire Series will noramilze by the *global* average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demean(df.review_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's de-mean each individual's reviews by their own average.\n",
    "This could be useful if, for example, you were building a recommendation system.\n",
    "A rating of 4 from someone's whose average is 2 is in some sense more meaningful that a 4 from someone who always gives 4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = df.groupby(\"profile_name\")[review_cols].transform(demean)\n",
    "normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `.transform` because the desired output was the same shape as the input.\n",
    "Just like `.agg` informs pandas that you want `1 input group → 1 output row`, the `.transform` method informs pandas that you want `1 input row → 1 output row`.\n",
    "\n",
    "`.transform` operates on each column independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Personal Trend?\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Personal Trend?</h1>\n",
    "</div>\n",
    "\n",
    "<p>Do reviewer's `review_overall` trend over a person's time reviewing?</p>\n",
    "\n",
    "Hint: Need an indictor that tracks which review this is for that person. That is, we need a cumulative count of reviews per person. - **(don't quite understand what this means.)**\n",
    "\n",
    "Implement `cumcount` to match the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumcount(s):\n",
    "    \"\"\"Returns an array with counting up to the length of 's'\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> cumcount([1, 2, 2, 1, 2])\n",
    "    array([0, 1, 2, 4])\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "cumcount([1, 2, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a variable `order` that has which review it was for that person.\n",
    "For example, if the raw reviews were like\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "      <th>Reviewer</th>\n",
    "      <th>Review Overall</th>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "      <tr>\n",
    "          <td>Alice</td>\n",
    "          <td>3</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Alice</td>\n",
    "          <td>3</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Bob</td>\n",
    "          <td>2</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Alice</td>\n",
    "          <td>4</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Bob</td>\n",
    "          <td>5</td>\n",
    "      </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "The `order` table would be\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "      <th>Reviewer</th>\n",
    "      <th>Order</th>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "      <tr>\n",
    "          <td>Alice</td>\n",
    "          <td>0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Alice</td>\n",
    "          <td>1</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Bob</td>\n",
    "          <td>0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Alice</td>\n",
    "          <td>2</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Bob</td>\n",
    "          <td>2</td>\n",
    "      </tr>\n",
    "  </tbody>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.groupby(\"profile_name\").review_overall.transform(...)\n",
    "order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what do we do with `order`? Hint: It's the same shape as `df` and we\n",
    "want to compute the average `review_overall` for all people with `order=0`,\n",
    "and all people with `order=1`, and `order=2`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/groupby_02.py\n",
    "order = df.groupby(\"profile_name\").review_overall.cumcount()\n",
    "df.groupby(order).review_overall.mean().plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General `.apply`\n",
    "\n",
    "We've seen `.agg` for outputting 1 row per group, and `.transform` for outputting 1 row per input row.\n",
    "\n",
    "The final kind of function application is `.apply`.\n",
    "This can do pretty much whatever you want. To be more precise, the groupby **`apply`** method can return a single value, a Series or a DataFrame. You must supply a custom function to **`apply`**. This custom function accepts the entire group as a **`DataFrame`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing a new dataset college\n",
    "college = pd.read_csv('data/college.csv')\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple examples to see how the groupby `apply` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_single(x):\n",
    "    return 'a single value'\n",
    "\n",
    "def return_series(x):\n",
    "    return pd.Series(data=['value 1', 'value 2'], index=['col A', 'col B'])\n",
    "\n",
    "def return_df(x):\n",
    "    return pd.DataFrame(np.random.rand(3,2), \n",
    "                        index=['row one', 'row two', 'row three'], \n",
    "                        columns=['col A', 'col B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.groupby(['STABBR', 'RELAFFIL']).apply(return_single).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.groupby(['STABBR', 'RELAFFIL']).apply(return_series).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.groupby(['STABBR', 'RELAFFIL']).apply(return_df).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass a function to `apply` that takes other arguments or keywords, you can pass these after the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one caveat is that `apply` may not be the fastest way. For instance, suppose we want to calculate the average SAT Math scores per state weighted by undergraduate population. There are two ways of finishing this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do some house cleaning\n",
    "college_drop = college[['STABBR', 'SATMTMID', 'UGDS']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit # Not using apply - calculate the weighted sums before groupby\n",
    "college_drop['MATH_WT'] = college_drop['SATMTMID'] * college_drop['UGDS']\n",
    "c1 = college_drop.groupby('STABBR')['MATH_WT', 'UGDS'].agg('sum')\n",
    "(c1['MATH_WT'] / c1['UGDS']).astype(int).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wa(df):\n",
    "    wa =  (df['SATMTMID'] * df['UGDS']).sum() / df['UGDS'].sum()\n",
    "    return wa.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit college_drop.groupby('STABBR').apply(calc_wa).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We used Python's iterator protocol to transform the raw data to a table\n",
    "- We saw how Dask could handle larger-than-memory data with a familiar API\n",
    "- We used groupby to analyze data by subsets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
